{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "# import tkinter\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting en reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0 # Start of sentence\n",
    "EOS_token = 1 # End of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def readLang(self, s):\n",
    "        for index, value in s.items():\n",
    "            value = normalizeString(value)\n",
    "            self.addSentence(value)\n",
    "\n",
    "    def showLang(self):\n",
    "        print(\"-- NL Language\")\n",
    "        print(\"Word count: \", sum(self.word2count.values()))\n",
    "        print(\"Vocabe size: \", self.n_words)\n",
    "        print(\"Example: \", self.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRLanguage:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(', '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def readLang(self, s):\n",
    "        for index, value in s.items():\n",
    "            value = normalizeMRString(value)\n",
    "            self.addSentence(value)\n",
    "\n",
    "    def showLang(self):\n",
    "        print(\"-- MR Language\")\n",
    "        print(\"Word count: \", sum(self.word2count.values()))\n",
    "        print(\"Vocabe size: \", self.n_words)\n",
    "        print(\"Example: \", self.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?_]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeMRString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r'\\[[^]]*\\]', r\"\", s) # TODO: Data in de square brackets is lost!!\n",
    "    # s = re.sub(r\"[\\[*\\]]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, delimeter=' '):\n",
    "    indices = [lang.word2index[word] for word in sentence.split(delimeter)]\n",
    "    return torch.tensor(indices)\n",
    "\n",
    "def sentenceFromIndexes(lang, indexes, delimeter=' '):\n",
    "    sentence = ''\n",
    "    for i in indexes:\n",
    "        sentence += str(lang.index2word[i.item()]) + delimeter\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def get_raw_data(file):\n",
    "    return pd.read_csv(file)[0:2000]\n",
    "\n",
    "def get_output_data(file):\n",
    "    corpus_df = pd.read_csv(file)[0:1900]\n",
    "    lang_df = corpus_df['ref']\n",
    "\n",
    "    natural_lang = Language()\n",
    "    natural_lang.readLang(lang_df)\n",
    "    # corpus_df['ref'] = corpus_df['ref'].astype(str) + ' eos'\n",
    "    corpus_df['ref'] = corpus_df['ref'].apply(lambda x: normalizeString(x))\n",
    "    corpus_df['ref'] = corpus_df['ref'].apply(lambda x: indexesFromSentence(natural_lang, x))\n",
    "\n",
    "    return corpus_df['ref'], natural_lang\n",
    "\n",
    "def get_input_data(file):\n",
    "    corpus_df = pd.read_csv(file)[0:1900]\n",
    "    lang_df = corpus_df['mr']\n",
    "\n",
    "    mr_lang = MRLanguage()\n",
    "    mr_lang.readLang(corpus_df['mr'])\n",
    "    # corpus_df['mr'] = corpus_df['mr'].astype(str) + ', eos'\n",
    "    corpus_df['mr'] = corpus_df['mr'].apply(lambda x: normalizeMRString(x))\n",
    "    corpus_df['mr'] = corpus_df['mr'].apply(lambda x: indexesFromSentence(mr_lang, x, delimeter=\", \"))\n",
    "\n",
    "    return corpus_df['mr'], mr_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "-- MR Language\n",
      "Word count:  10103\n",
      "Vocabe size:  12\n",
      "Example:  {'name[name_x]': 1900, 'eattype[eattype_x]': 894, 'pricerange[pricerange_x]': 1287, 'customer rating[customer rating_x out of 5]': 146, 'near[near_x]': 913, 'food[food_x]': 1574, 'customer rating[customer rating_x]': 1105, 'area[area_x]': 1089, 'familyfriendly[yes]': 818, 'familyfriendly[no]': 377}\n",
      "None\n",
      "-- NL Language\n",
      "Word count:  30136\n",
      "Vocabe size:  661\n",
      "Example:  {'name_x': 1626, 'eattype_x': 677, 'near': 738, 'near_x': 893, 'has': 450, 'a': 1563, 'customer': 1705, 'rating_x': 1089, 'star': 67, 'rating': 642, 'prices': 126, 'pricerange_x': 844, 'close': 31, 'to': 125, 'serves': 292, 'food_x': 1484, 'tuscan': 1, 'beef': 1, 'for': 140, 'the': 1150, 'price': 452, 'of': 460, 'delicious': 11, 'pub': 8, 'food': 647, '': 1527, 'is': 1905, 'rated': 158, 'and': 662, 'area_x': 1059, 'that': 260, 'familyfriendly_x': 958, 'located': 533, 'theme': 1, 'eatery': 10, 'called': 174, 'with': 396, 'range': 410, 'it': 690, 'in': 816, 'area': 201, 'by': 111, 'pricerange_xly': 59, 'priced': 189, 'restaurant': 324, 'who': 1, 's': 56, 'main': 2, 'cuisine': 39, 'you': 58, 'will': 18, 'find': 24, 'this': 40, 'local': 3, 'gem': 1, 'high': 42, 'situated': 25, 'at': 106, 'are': 79, 'adults': 8, 'place': 70, 'they': 59, 'have': 38, 'pricing': 4, 'can': 43, 'serve': 21, 'however': 8, 'only': 17, 'an': 235, 'provides': 53, 'breakfast': 6, 'joint': 6, 'offers': 75, 'though': 4, 'poor': 10, 'their': 16, 'its': 63, 'rating_xly': 35, 'customers': 54, 'average': 47, 'but': 65, 'there': 142, 'which': 61, 'out': 148, 'features': 3, 'ratings': 64, 'serving': 75, 'received': 5, 'reviews': 17, 'providing': 24, 'dine': 1, 'low': 77, 'named': 55, 'city': 89, 'centre': 50, 'cheap': 24, 'should': 2, 'be': 34, 'noted': 1, 'pounds': 5, 'ranging': 7, 'non': 8, 'fine': 2, 'not': 96, 'quite': 4, 'while': 8, 'recommended': 9, 'on': 76, 'offering': 28, 'rating_xer': 3, 'pay': 4, 'over': 8, 'good': 22, 'eattype_xlic': 1, 'house': 9, 'one': 12, 'inexpensive': 11, 'north': 6, 'fast': 6, 'fairly': 2, 'excellent': 7, 'some': 6, 'lovely': 2, 'stars': 29, 'no': 14, 'children': 20, 'please': 2, 'was': 4, 'rate': 11, 'wine': 11, 'cheese': 8, 'cooks': 2, 'yet': 2, 'been': 8, 'served': 6, 'budget': 4, 'spirits': 4, 'relax': 1, 'river': 23, 'between': 8, 'environment': 8, 'as': 25, 'traditional': 6, 'costs': 8, 'services': 2, 'isn': 10, 'shop': 17, 'point': 2, 'go': 6, 'very': 16, 'five': 14, 'averagely': 3, 'british': 10, 'next': 19, 'welcome': 5, 'because': 4, 'mid': 15, 'fare': 5, 'twenty': 6, 'pound': 2, 'just': 8, 'road': 4, 'from': 23, 'running': 1, 'surprising': 1, 'cost': 18, 'right': 7, 'found': 22, 'conveniently': 2, 'town': 1, 'name_xs': 1, 'style': 14, 'classified': 1, 'if': 21, 're': 11, 'upscale': 2, 'look': 5, 'further': 3, 'than': 4, 'coffee': 8, 'affordable': 15, 'generally': 2, 'pleased': 2, 'given': 8, 'great': 29, 'simple': 1, 'lunch': 3, 'work': 1, 'meetings': 1, 'or': 4, 'friends': 1, 'fair': 4, 'featuring': 2, 'highly': 24, 'items': 3, 'also': 23, 'boasts': 4, 'being': 5, 'kids': 17, 'alcustomer': 3, 'rating_xed': 3, 'establishment': 11, 'satisfaction': 2, 'nearby': 9, 'allowed': 4, 'hotel': 6, 'part': 1, 'chain': 1, 'end': 10, 'standard': 1, 'dishes': 3, 'offer': 4, 'permitted': 1, 'higher': 3, 'outskirts': 1, 'name': 5, 'fitzbillies': 2, 'does': 13, 'expensive': 39, 'few': 1, 'pricerange_xs': 1, 'clients': 1, 't': 9, 'sells': 19, 'cheeses': 2, 'fondues': 1, 'enjoy': 4, 'atmosphere': 12, 'all': 8, 'midsummer': 6, 'options': 4, 'venue': 18, 'best': 5, 'costing': 6, 'visit': 6, 'within': 8, 'breakfasts': 2, 'ranged': 2, 'get': 10, 'when': 2, 'exceptional': 1, 'allow': 6, 'review': 5, 'location': 9, 'costumer': 4, 'do': 3, 'where': 5, 'alimentum': 2, 'burger': 2, 'well': 9, 'gourmet': 1, 'welcomes': 2, 'french': 2, 'locate': 1, 'll': 1, 'whose': 3, 'meals': 9, 'cater': 4, 'families': 5, 'drinks': 3, 'suitable': 10, 'looking': 12, 'option': 5, 'decent': 5, 'current': 1, 'whilst': 1, 'try': 20, 'public': 2, 'dumpling': 1, 'tree': 1, 'matter': 1, 'noodles': 1, 'specialty': 2, 'popular': 2, 'too': 2, 'thinking': 1, 'taking': 1, 'possible': 1, 'beside': 1, 'possesses': 1, 'around': 5, 'locates': 1, 'although': 10, 'wide': 2, 'delivers': 1, 'fantastic': 1, 'service': 7, 'young': 2, 'up': 6, 'servings': 1, 'receives': 4, 'mile': 1, 'our': 7, 'three': 11, 'choice': 4, 'drink': 1, 'family': 10, 'old': 1, 'under': 6, 'food_xs': 2, 'selling': 1, 'sushi': 13, 'less': 1, 'block': 1, 'away': 8, 'along': 13, 'same': 1, 'age': 3, 'groups': 3, 'store': 4, 'quiet': 1, 'minors': 1, 'we': 9, 'doesn': 5, 'eat': 11, 'moderately': 10, 'medium': 9, 'starting': 3, 'arms': 2, 'both': 2, 'frugally': 1, 'take': 7, 'after': 2, 'caters': 1, 'mood': 3, 'currently': 2, 'pasta': 2, 'center': 10, 'reasonable': 1, 'dulwich': 1, 'nice': 5, 'fresh': 1, 'foods': 6, 'visitor': 1, 'unfortunately': 2, 'friendly': 5, 'affordably': 2, 'opened': 3, 'rates': 4, 'menu': 10, 'bit': 5, 'considered': 2, 'type': 3, 'know': 1, 'cotto': 2, 'middle': 4, 'ranges': 5, 'want': 5, 'your': 5, 'hamburgers': 2, 'down': 5, 'gets': 3, 'wines': 5, 'bought': 1, 'neat': 4, 'euros': 8, 'welcoming': 3, 'outstanding': 1, 'absolutely': 1, 'love': 2, 'scoring': 2, 'somewhere': 1, 'i': 8, 'think': 1, 'about': 3, 'lot': 2, 'pricey': 3, 'perfect': 2, 'cheaply': 1, 'strada': 2, 'picnic': 2, 'lunches': 1, 'terrible': 2, 'quality': 13, 'check': 2, 'pricerange_xer': 2, 'wildwood': 2, 'light': 1, 'meal': 6, 'variety': 1, 'native': 1, 'outside': 6, 'far': 2, 'so': 3, 'waterman': 2, 'regularly': 1, 'pleasant': 1, 'specializes': 3, 'yes': 9, 'riverside': 3, 'zizzi': 1, 'cambridge': 4, 'platters': 1, 'fondue': 2, 'level': 2, 'large': 1, 'clientele': 2, 'eaten': 1, 'entries': 2, 'brew': 1, 'provide': 1, 'happy': 1, 'sit': 1, 'grapes': 1, 'requires': 1, 'tasting': 1, 'let': 1, 'value': 2, 'got': 5, 'thing': 1, 'sits': 1, 'scores': 3, 'reasonably': 5, 'adult': 5, 'having': 1, 'below': 1, 'upper': 1, 'selection': 2, 'cuisines': 1, 'asian': 1, 'moderate': 4, 'beautiful': 3, 'small': 2, 'world': 1, 'feedback': 1, 'alongside': 1, 'heard': 1, 'dining': 5, 'really': 4, 'waterfront': 1, 'sample': 1, 'remember': 1, 'servers': 2, 'keeping': 1, 'consumer': 5, 'spot': 2, 'american': 2, 'building': 1, 'keep': 1, 'tied': 1, 'dinner': 3, 'names': 1, 'expect': 2, 'off': 1, 'start': 2, 'lies': 2, 'more': 7, 'chinese': 1, 'boat': 6, 'above': 4, 'indian': 1, 'seeking': 1, 'us': 3, 'centrally': 1, 'most': 1, 'offerings': 1, 'grab': 1, 'themed': 1, 'two': 1, 'poorly': 2, 'acclaimed': 1, 'steeply': 1, 'standards': 1, 'costly': 1, 'facility': 1, 'bracket': 2, 'starred': 1, 'plates': 1, 'ice': 1, 'quick': 1, 'make': 1, 'join': 1, 'aromi': 1, 'gave': 1, 'takeout': 2, 'made': 1, 'honorable': 1, 'mention': 1, 'relatively': 1, 'lives': 1, 'his': 1, 'parents': 1, 'he': 3, 'loves': 1, 'going': 1, 'restaurants': 1, 'inside': 1, 'playgrounds': 1, 'why': 1, 'street': 1, 'golden': 2, 'curry': 2, 'conducive': 2, 'without': 1, 'hors': 1, 'd': 1, 'due': 1, 'quaint': 1, 'wall': 1, 'craving': 1, 'couldn': 1, 'any': 1, 'better': 1, 'steal': 1, 'rage': 2, 'specializing': 1, 'olive': 1, 'grove': 1, 'rave': 1, 'would': 2, 'recommend': 2, 'day': 3, 'water': 1, 'play': 1, 'billiard': 1, 'welcomed': 2, 'typical': 1, 'fish': 1, 'heart': 1, 'little': 1, 'ccustomer': 1, 'rating_xns': 1, 'everyday': 1, 'via': 1, 'bar': 2, 'lowly': 1, 'reflect': 1, 'mature': 1, 'competitive': 1, 'dish': 1, 'money': 1, 'rice': 4, 'earned': 1, 'stunning': 1, 'view': 3, 'attractive': 1, 'warm': 1, 'burgers': 1, 'fries': 1, 'other': 2, 'fatty': 1, 'snacks': 2, 'allows': 1, 'rank': 1, 'new': 1, 'views': 1, 'come': 1, 'tasty': 3, 'italian': 1, 'typically': 1, 'charges': 1, 'superb': 1, 'across': 1, 'home': 2, 'boost': 1, 'today': 1, 'score': 1, 'phoenix': 2, 'rater': 1, 'comes': 1, 'bad': 1, 'ok': 1, 'shopping': 3, 'entertainment': 1, 'hotels': 1, 'rating_xways': 1, 'carryout': 1, 'chopsticks': 1, 'available': 1, 'expert': 1, 'give': 6, 'towards': 1, 'reviewed': 2, 'known': 1, 'includes': 2, 'must': 1, 'modest': 1, 'fans': 1, 'arrangement': 1, 'english': 2, 'mcdonalds': 1, 'overlooking': 1, 'ideal': 1, 'setting': 3, 'choose': 1, 'worth': 1, 'had': 1, 'super': 1, 'ever': 1, 'won': 1, 'break': 1, 'then': 4, 'japanese': 1, 'recently': 1, 'scored': 1, 'parties': 1, 'ranked': 1, 'another': 1, 'falls': 1, 'here': 1, 'vaults': 1, 'amazing': 1, 'improving': 1, 'places': 1, 'beluga': 1, 'cam': 1, 'seafood': 1, 'exceeding': 1, 'focus': 1, 'law': 1, 'traveling': 1, 'provided': 1, 'trip': 1, 'able': 1, 'irresistible': 1, 'combo': 1, 'calm': 1, 'hungry': 1, 'beast': 1, 'including': 1, 'per': 1, 'head': 1, 'mark': 1, 'sorry': 1, 'treat': 1, 'yourself': 1, 'outlet': 1, 'prince': 1, 'might': 1, 'inn': 1, 'cricketers': 1, 'child': 1, 'cafe': 1, 'achieved': 1, 'spend': 1, 'upwards': 1, 'like': 1, 'rear': 1, 'opposite': 1, 'cocum': 1, 'pastas': 1, 'short': 1, 'distance': 1, 'highest': 1, 'takeaway': 2, 'experience': 1, 'urban': 1, 'middling': 1, 'soup': 1, 'spending': 1, 'stone': 1, 'throw': 1, 'stay': 1, 'mostly': 1, 'hearty': 1, 'previous': 1, 'said': 1, 'hall': 1, 'noodle': 1}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# GET DATA\n",
    "print(\"Reading...\")\n",
    "path = \"delexicalized/delex_only.csv\"\n",
    "raw_data = get_raw_data(path)\n",
    "input_data, mr_lang = get_input_data(path) #TODO: Correctly read all info of labels\n",
    "target_data, nl_lang = get_output_data(path)\n",
    "\n",
    "print(mr_lang.showLang())\n",
    "print(nl_lang.showLang())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=75):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=75):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):   \n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epoch, stop_early_count, learning_rate=0.01):\n",
    "    print(\"Started training!\")\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    best_loss = None # Keep track for early stopping\n",
    "    no_improv_count = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate) # Add Adam\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate) \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        for i in range(len(input_data)):\n",
    "            input_tensor = input_data[i].view(-1, 1)\n",
    "            target_tensor = target_data[i].view(-1, 1)\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            \n",
    "        print_loss_avg = print_loss_total / len(input_data)\n",
    "        \n",
    "        if best_loss == None or best_loss > print_loss_avg:\n",
    "            best_loss = print_loss_avg\n",
    "            no_improv_count = 0\n",
    "        else:\n",
    "            no_improv_count += 1\n",
    "            \n",
    "        \n",
    "        print_loss_total = 0\n",
    "        print('%s (Epoch: %d %d%%) %.4f' % (timeSince(start, epoch / n_epoch),\n",
    "                                     epoch, epoch / n_epoch * 100, print_loss_avg))\n",
    "\n",
    "        plot_loss_avg = plot_loss_total / len(input_data)\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        if no_improv_count == stop_early_count:\n",
    "                print(\"Stopping early...\")\n",
    "                break\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    print(\"Done!\")\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length=75):\n",
    "    with torch.no_grad():\n",
    "#         sentence = sentenceFromIndexes(mr_lang, input_tensor, delimeter=\", \")\n",
    "#         input_tensor = indexesFromSentence(mr_lang, sentence, delimeter=\", \")\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(nl_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0, len(input_data) - 1)\n",
    "        input_sentence = sentenceFromIndexes(mr_lang, input_data[index], delimeter=\", \")\n",
    "        target_sentence = sentenceFromIndexes(nl_lang, target_data[index], delimeter=\" \")\n",
    "        \n",
    "        print('>', input_sentence)\n",
    "        print('=', target_sentence)\n",
    "        output_words, attentions = evaluate(encoder, decoder, input_data[i])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training!\n",
      "3m 8s (- 310m 16s) (Epoch: 1 1%) 5.7289\n",
      "7m 46s (- 381m 13s) (Epoch: 2 2%) 6.3255\n",
      "12m 6s (- 391m 20s) (Epoch: 3 3%) 6.4452\n",
      "16m 34s (- 397m 42s) (Epoch: 4 4%) 6.5796\n",
      "20m 56s (- 397m 49s) (Epoch: 5 5%) 6.4475\n",
      "25m 24s (- 397m 57s) (Epoch: 6 6%) 6.5040\n",
      "30m 5s (- 399m 46s) (Epoch: 7 7%) 6.6465\n",
      "34m 58s (- 402m 11s) (Epoch: 8 8%) 6.6491\n",
      "39m 52s (- 403m 7s) (Epoch: 9 9%) 6.7872\n",
      "44m 46s (- 403m 0s) (Epoch: 10 10%) 6.6820\n",
      "49m 36s (- 401m 24s) (Epoch: 11 11%) 6.8482\n",
      "54m 28s (- 399m 32s) (Epoch: 12 12%) 6.9099\n",
      "59m 34s (- 398m 38s) (Epoch: 13 13%) 6.8976\n",
      "64m 45s (- 397m 47s) (Epoch: 14 14%) 7.0128\n",
      "69m 51s (- 395m 53s) (Epoch: 15 15%) 6.8493\n",
      "74m 57s (- 393m 29s) (Epoch: 16 16%) 7.1146\n",
      "80m 3s (- 390m 50s) (Epoch: 17 17%) 7.1875\n",
      "85m 4s (- 387m 35s) (Epoch: 18 18%) 7.0704\n",
      "90m 4s (- 383m 58s) (Epoch: 19 19%) 7.0811\n",
      "95m 13s (- 380m 53s) (Epoch: 20 20%) 7.0852\n",
      "100m 15s (- 377m 8s) (Epoch: 21 21%) 7.2614\n",
      "105m 17s (- 373m 16s) (Epoch: 22 22%) 7.1164\n",
      "110m 16s (- 369m 9s) (Epoch: 23 23%) 7.0087\n",
      "115m 9s (- 364m 40s) (Epoch: 24 24%) 7.0344\n",
      "119m 58s (- 359m 55s) (Epoch: 25 25%) 7.0947\n",
      "124m 45s (- 355m 4s) (Epoch: 26 26%) 7.1938\n",
      "Stopping early...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "early_stopping = 25\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(mr_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, nl_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "plot_losses = trainIters(encoder1, attn_decoder1, n_epoch, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> name[name_x], eattype[eattype_x], food[food_x], area[area_x], \n",
      "= name_x is a eattype_x in area_x that offers food_x  \n",
      "< near near_x there eattype_x a eattype_x with a customer rating of customer rating_x out of customer name_x  a a a a of customer                          a pricerange_x the customer       the     for a low the customer rating and is located near the\n",
      "\n",
      "> name[name_x], food[food_x], pricerange[pricerange_x], familyfriendly[no], near[near_x], \n",
      "= name_x is a familyfriendly_x restaurant it does not allow children and is food_x near to near_x  \n",
      "< name_x is a eattype_x near_x food_x near near_x located near_x                                 for a price range of pricerange_x  pricerange_x the pricerange_x are near the near_x  it it it             the  \n",
      "\n",
      "> name[name_x], food[food_x], pricerange[pricerange_x], familyfriendly[yes], \n",
      "= name_x is a food_x restaurant that is familyfriendly_x a price range of pricerange_x  \n",
      "< name_x is a familyfriendly_x a customer rating_x area_x range it serves food_x food has a price range pricerange_x customers give it an customer rating_x rating  customer customer customer customer customer customer customer customer  customer   customer                                    \n",
      "\n",
      "> name[name_x], eattype[eattype_x], food[food_x], customer rating[customer rating_x], area[area_x], \n",
      "= name_x is a eattype_x that sells food_x food and is located in area_x and has a customer rating of customer rating_x  \n",
      "< name_x is a area_x near near_x eattype_x a a a a a a price range pricerange_x near_x                                         the          the       \n",
      "\n",
      "> name[name_x], food[food_x], customer rating[customer rating_x], area[area_x], familyfriendly[yes], \n",
      "= name_x has an customer rating_x rating and offers food_x cuisine area_x to eat outside of the city centre  \n",
      "< name_x is customer rating_x food_x area_x near near_x    customer   customer                      all good option  the customer customer  customer                for a good option it is located in the area_x area and is rated customer\n",
      "\n",
      "> name[name_x], eattype[eattype_x], food[food_x], customer rating[customer rating_x], near[near_x], \n",
      "= name_x is a eattype_x with a customer rating_x customer rating that serves food_x food near near_x  \n",
      "< name_x is a area_x priced pricerange_x a near restaurant a    near near_x  it                      the  area    the          the    the          the      \n",
      "\n",
      "> name[name_x], eattype[eattype_x], pricerange[pricerange_x], customer rating[customer rating_x], familyfriendly[yes], near[near_x], \n",
      "= name_x eattype_x is near near_x it is familyfriendly_x with a price range of pricerange_x customers have given it an customer rating_x rating  \n",
      "< name_x provides food_x food it has a customer rating of customer rating_x it is located in the area_x area and is not familyfriendly_x                   for children  the rating_x rating_x rating_x rating_x customer rating_x                       the \n",
      "\n",
      "> name[name_x], eattype[eattype_x], food[food_x], pricerange[pricerange_x], area[area_x], familyfriendly[yes], \n",
      "= name_x eattype_x in the area_x area serves food_x food it is familyfriendly_x and the price range is pricerange_x  \n",
      "< name_x is a eattype_x area_x near_x a food_x area_x area_x the a a price range pricerange_x it it near_x  it familyfriendly_x                          the   area          the  area        the    \n",
      "\n",
      "> name[name_x], pricerange[pricerange_x], near[near_x], \n",
      "= pricerange_x priced restaurant in the city centre near_x \n",
      "< name_x is a eattype_x eattype_x located in the area_x customer rating_x customer rating_x rating customer rating_x rating_x rating_x rating_x rating_x  familyfriendly_x  food_x                          for a dinner  to the city centre  customer rating_x customer rating_x rating_x customer customer rating_x rating_x rating  customer     \n",
      "\n",
      "> name[name_x], food[food_x], customer rating[customer rating_x out of 5], area[area_x], familyfriendly[no], \n",
      "= name_x rated customer rating_x out of is a familyfriendly_x food_x in the area_x \n",
      "< name_x is a food_x food_x area_x area_x priced priced customer rating_x customer rating_x a customer customer  customer  customer                                  the               the      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This should actually use a validation set!!\n",
    "def generateOutputFile(encoder, decoder, file_name, amount=20):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "    \n",
    "        for i in range(100):\n",
    "            index = i + 1800 #random.randint(0, len(input_data) - 1)\n",
    "            \n",
    "            inputs.append(raw_data['mr'][index])\n",
    "\n",
    "            \n",
    "            input_sentence = sentenceFromIndexes(mr_lang, input_data[index], delimeter=\", \")\n",
    "            target_sentence = sentenceFromIndexes(nl_lang, target_data[index], delimeter=\" \")\n",
    "\n",
    "            output_words, attentions = evaluate(encoder, decoder, input_data[i])\n",
    "            output_sentence = ' '.join(output_words)\n",
    "            outputs.append(output_sentence)\n",
    "        \n",
    "        results = {\n",
    "            'mr': inputs,\n",
    "            'output': outputs\n",
    "        }\n",
    "        \n",
    "        df_results = pd.DataFrame(results, columns=['mr','output'])\n",
    "        df_results.to_csv(file_name, encoding='utf-8', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateOutputFile(encoder1, attn_decoder1, \"./delex_nl_output.csv\", amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
