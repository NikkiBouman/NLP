{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "# import tkinter\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting en reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0 # Start of sentence\n",
    "EOS_token = 1 # End of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def readLang(self, s):\n",
    "        for index, value in s.items():\n",
    "            value = normalizeString(value)\n",
    "            self.addSentence(value)\n",
    "\n",
    "    def showLang(self):\n",
    "        print(\"-- NL Language\")\n",
    "        print(\"Word count: \", sum(self.word2count.values()))\n",
    "        print(\"Vocabe size: \", self.n_words)\n",
    "        print(\"Example: \", self.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRLanguage:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(', '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def readLang(self, s):\n",
    "        for index, value in s.items():\n",
    "            value = normalizeMRString(value)\n",
    "            self.addSentence(value)\n",
    "\n",
    "    def showLang(self):\n",
    "        print(\"-- MR Language\")\n",
    "        print(\"Word count: \", sum(self.word2count.values()))\n",
    "        print(\"Vocabe size: \", self.n_words)\n",
    "        print(\"Example: \", self.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?_]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeMRString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r'\\[[^]]*\\]', r\"\", s) # TODO: Data in de square brackets is lost!!\n",
    "    # s = re.sub(r\"[\\[*\\]]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, delimeter=' '):\n",
    "    indices = [lang.word2index[word] for word in sentence.split(delimeter)]\n",
    "    return torch.tensor(indices)\n",
    "\n",
    "def sentenceFromIndexes(lang, indexes, delimeter=' '):\n",
    "    sentence = ''\n",
    "    for i in indexes:\n",
    "        sentence += str(lang.index2word[i.item()]) + delimeter\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def get_raw_data(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def get_output_data(file):\n",
    "    corpus_df = pd.read_csv(file)\n",
    "    lang_df = corpus_df['ref']\n",
    "\n",
    "    natural_lang = Language()\n",
    "    natural_lang.readLang(lang_df)\n",
    "    # corpus_df['ref'] = corpus_df['ref'].astype(str) + ' eos'\n",
    "    corpus_df['ref'] = corpus_df['ref'].apply(lambda x: normalizeString(x))\n",
    "    corpus_df['ref'] = corpus_df['ref'].apply(lambda x: indexesFromSentence(natural_lang, x))\n",
    "\n",
    "    return corpus_df['ref'], natural_lang\n",
    "\n",
    "def get_input_data(file):\n",
    "    corpus_df = pd.read_csv(file)\n",
    "    lang_df = corpus_df['mr']\n",
    "\n",
    "    mr_lang = MRLanguage()\n",
    "    mr_lang.readLang(corpus_df['mr'])\n",
    "    # corpus_df['mr'] = corpus_df['mr'].astype(str) + ', eos'\n",
    "    corpus_df['mr'] = corpus_df['mr'].apply(lambda x: normalizeMRString(x))\n",
    "    corpus_df['mr'] = corpus_df['mr'].apply(lambda x: indexesFromSentence(mr_lang, x, delimeter=\", \"))\n",
    "\n",
    "    return corpus_df['mr'], mr_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "-- MR Language\n",
      "Word count:  10632\n",
      "Vocabe size:  12\n",
      "Example:  {'name[name_x]': 1998, 'eattype[eattype_x]': 937, 'pricerange[pricerange_x]': 1354, 'customer rating[customer rating_x out of 5]': 158, 'near[near_x]': 964, 'food[food_x]': 1660, 'customer rating[customer rating_x]': 1162, 'area[area_x]': 1143, 'familyfriendly[yes]': 865, 'familyfriendly[no]': 391}\n",
      "None\n",
      "-- NL Language\n",
      "Word count:  32610\n",
      "Vocabe size:  671\n",
      "Example:  {'name_x': 1711, 'eattype_x': 712, 'near': 780, 'near_x': 944, 'has': 483, 'a': 1642, 'customer': 1797, 'rating_x': 1146, 'star': 71, 'rating': 680, '.': 2504, 'prices': 131, 'pricerange_x': 886, 'close': 31, 'to': 133, 'serves': 308, 'food_x': 1567, 'tuscan': 1, 'beef': 1, 'for': 145, 'the': 1206, 'price': 469, 'of': 487, 'delicious': 11, 'pub': 9, 'food': 687, 'is': 1999, 'rated': 164, 'and': 691, 'area_x': 1113, 'that': 271, 'familyfriendly_x': 1009, 'located': 563, 'theme': 1, 'eatery': 11, 'called': 186, 'with': 413, 'range': 425, 'it': 730, 'in': 854, 'area': 211, 'by': 116, 'pricerange_xly': 62, 'priced': 200, 'restaurant': 344, 'who': 1, 's': 60, 'main': 2, 'cuisine': 44, 'you': 59, 'will': 18, 'find': 24, 'this': 42, 'local': 3, 'gem': 1, 'high': 43, 'situated': 28, 'at': 111, 'are': 89, 'adults': 9, 'place': 73, 'they': 65, 'have': 41, 'pricing': 5, 'can': 43, 'serve': 23, 'however': 8, 'only': 17, 'an': 251, 'provides': 60, 'breakfast': 7, 'joint': 6, 'offers': 79, 'though': 4, 'poor': 10, 'their': 18, 'its': 65, 'rating_xly': 36, 'customers': 55, 'average': 47, 'but': 68, 'there': 151, 'which': 66, 'out': 161, 'features': 3, 'ratings': 67, 'serving': 80, 'received': 5, 'reviews': 17, 'providing': 25, 'dine': 2, 'low': 79, 'named': 56, 'city': 93, 'centre': 52, 'cheap': 25, 'should': 2, 'be': 34, 'noted': 1, 'pounds': 5, 'ranging': 7, 'non': 8, 'fine': 2, 'not': 100, 'quite': 4, 'while': 8, 'recommended': 9, 'on': 80, 'offering': 29, 'rating_xer': 3, 'pay': 4, 'over': 8, 'good': 25, 'eattype_xlic': 1, 'house': 9, 'one': 13, 'inexpensive': 12, 'north': 7, 'fast': 7, 'fairly': 3, 'excellent': 7, 'some': 6, 'lovely': 2, 'stars': 30, 'no': 16, 'children': 20, 'please': 2, 'was': 4, 'rate': 11, 'wine': 11, 'cheese': 8, 'cooks': 2, 'yet': 2, 'been': 9, 'served': 6, 'budget': 4, 'spirits': 4, 'relax': 1, 'river': 26, 'between': 8, 'environment': 8, 'as': 25, 'traditional': 6, 'costs': 8, 'services': 2, 'isn': 11, 'shop': 17, 'point': 2, 'go': 7, 'very': 16, 'five': 15, 'averagely': 3, 'british': 12, 'next': 19, 'welcome': 5, 'because': 4, 'mid': 16, 'fare': 5, 'twenty': 6, 'pound': 2, 'just': 8, 'road': 4, 'from': 24, 'running': 1, 'surprising': 1, 'cost': 19, 'right': 8, 'found': 22, 'conveniently': 2, 'town': 1, 'name_xs': 1, 'style': 14, 'classified': 1, 'if': 22, 're': 11, 'upscale': 2, 'look': 6, 'further': 4, 'than': 5, 'coffee': 8, 'affordable': 17, 'generally': 2, 'pleased': 2, 'given': 8, 'great': 29, 'simple': 1, 'lunch': 3, 'work': 1, 'meetings': 1, 'or': 4, 'friends': 1, 'fair': 4, 'featuring': 2, 'highly': 25, 'items': 4, 'also': 24, 'boasts': 4, 'being': 5, 'kids': 17, 'alcustomer': 3, 'rating_xed': 3, 'establishment': 12, 'satisfaction': 3, 'nearby': 9, 'allowed': 4, 'hotel': 6, 'part': 1, 'chain': 1, 'end': 10, 'standard': 1, 'dishes': 4, 'offer': 4, 'permitted': 1, 'higher': 4, 'outskirts': 1, 'name': 5, 'fitzbillies': 3, 'does': 13, 'expensive': 41, 'few': 1, 'pricerange_xs': 1, 'clients': 1, 't': 9, 'sells': 20, 'cheeses': 2, 'fondues': 1, 'enjoy': 5, 'atmosphere': 12, 'all': 8, 'midsummer': 6, 'options': 4, 'venue': 18, 'best': 5, 'costing': 6, 'visit': 6, 'within': 8, 'breakfasts': 2, 'ranged': 2, 'get': 10, 'when': 2, 'exceptional': 1, 'allow': 6, 'review': 5, 'location': 9, 'costumer': 4, 'do': 3, 'where': 5, 'alimentum': 2, 'burger': 2, 'well': 9, 'gourmet': 1, 'welcomes': 2, 'french': 2, 'locate': 1, 'll': 1, 'whose': 3, 'meals': 9, 'cater': 4, 'families': 5, 'drinks': 3, 'suitable': 10, 'looking': 12, 'option': 5, 'decent': 5, 'current': 1, 'whilst': 1, 'try': 20, 'public': 3, 'dumpling': 1, 'tree': 1, 'matter': 1, 'noodles': 1, 'specialty': 2, 'popular': 2, 'too': 2, 'thinking': 1, 'taking': 1, 'possible': 1, 'beside': 1, 'possesses': 1, 'around': 5, 'locates': 1, 'although': 11, 'wide': 2, 'delivers': 1, 'fantastic': 1, 'service': 7, 'young': 2, 'up': 6, 'servings': 1, 'receives': 4, 'mile': 1, 'our': 7, 'three': 11, 'choice': 4, 'drink': 1, 'family': 10, 'old': 1, 'under': 6, 'food_xs': 2, 'selling': 2, 'sushi': 15, 'less': 1, 'block': 1, 'away': 8, 'along': 14, 'same': 1, 'age': 3, 'groups': 3, 'store': 4, 'quiet': 1, 'minors': 1, 'we': 12, 'doesn': 5, 'eat': 13, 'moderately': 10, 'medium': 10, 'starting': 4, 'arms': 3, 'both': 2, 'frugally': 1, 'take': 7, 'after': 2, 'caters': 2, 'mood': 4, 'currently': 2, 'pasta': 2, 'center': 12, 'reasonable': 3, 'dulwich': 1, 'nice': 5, 'fresh': 1, 'foods': 6, 'visitor': 1, 'unfortunately': 2, 'friendly': 5, 'affordably': 2, 'opened': 3, 'rates': 4, 'menu': 11, 'bit': 5, 'considered': 2, 'type': 4, 'know': 1, 'cotto': 2, 'middle': 4, 'ranges': 5, 'want': 5, 'your': 5, 'hamburgers': 2, 'down': 5, 'gets': 3, 'wines': 5, 'bought': 1, 'neat': 4, 'euros': 8, 'welcoming': 3, 'outstanding': 1, 'absolutely': 1, 'love': 2, 'scoring': 2, 'somewhere': 1, 'i': 8, 'think': 1, 'about': 3, 'lot': 2, 'pricey': 3, 'perfect': 3, 'cheaply': 1, 'strada': 2, 'picnic': 2, 'lunches': 1, 'terrible': 2, 'quality': 13, 'check': 2, 'pricerange_xer': 2, 'wildwood': 2, 'light': 1, 'meal': 6, 'variety': 1, 'native': 1, 'outside': 7, 'far': 2, 'so': 3, 'waterman': 2, 'regularly': 1, 'pleasant': 1, 'specializes': 3, 'yes': 9, 'riverside': 3, 'zizzi': 1, 'cambridge': 4, 'platters': 1, 'fondue': 2, 'level': 2, 'large': 1, 'clientele': 2, 'eaten': 1, 'entries': 2, 'brew': 1, 'provide': 1, 'happy': 1, 'sit': 1, 'grapes': 1, 'requires': 1, 'tasting': 1, 'let': 1, 'value': 2, 'got': 5, 'thing': 1, 'sits': 1, 'scores': 3, 'reasonably': 5, 'adult': 5, 'having': 1, 'below': 1, 'upper': 1, 'selection': 2, 'cuisines': 1, 'asian': 1, 'moderate': 5, 'beautiful': 3, 'small': 2, 'world': 1, 'feedback': 1, 'alongside': 1, 'heard': 1, 'dining': 5, 'really': 4, 'waterfront': 1, 'sample': 1, 'remember': 1, 'servers': 2, 'keeping': 1, 'consumer': 5, 'spot': 3, 'american': 2, 'building': 1, 'keep': 1, 'tied': 1, 'dinner': 3, 'names': 1, 'expect': 2, 'off': 1, 'start': 2, 'lies': 2, 'more': 7, 'chinese': 1, 'boat': 7, 'above': 4, 'indian': 2, 'seeking': 1, 'us': 3, 'centrally': 1, 'most': 1, 'offerings': 1, 'grab': 1, '': 9, 'themed': 1, 'two': 1, 'poorly': 2, 'acclaimed': 1, 'steeply': 1, 'standards': 1, 'costly': 1, 'facility': 1, 'bracket': 2, 'starred': 1, 'plates': 1, 'ice': 1, 'quick': 1, 'make': 2, 'join': 1, 'aromi': 1, 'gave': 1, 'takeout': 2, 'made': 1, 'honorable': 1, 'mention': 1, 'relatively': 1, 'lives': 1, 'his': 1, 'parents': 1, 'he': 3, 'loves': 1, 'going': 1, 'restaurants': 1, 'inside': 1, 'playgrounds': 1, 'why': 1, 'street': 1, 'golden': 2, 'curry': 2, 'conducive': 2, 'without': 1, 'hors': 1, 'd': 1, 'due': 1, 'quaint': 1, 'wall': 1, 'craving': 1, 'couldn': 1, 'any': 1, 'better': 1, 'steal': 1, 'rage': 2, 'specializing': 1, 'olive': 1, 'grove': 1, 'rave': 1, 'would': 2, 'recommend': 2, 'day': 3, 'water': 1, 'play': 1, 'billiard': 1, '.familyfriendly_x': 2, 'welcomed': 2, 'typical': 1, 'fish': 1, 'heart': 1, 'little': 1, 'ccustomer': 1, 'rating_xns': 1, 'everyday': 1, 'via': 1, 'bar': 2, 'lowly': 1, 'reflect': 1, 'mature': 1, 'competitive': 1, 'dish': 1, 'money': 1, 'rice': 5, 'earned': 1, 'stunning': 1, 'view': 3, 'attractive': 1, 'warm': 1, 'burgers': 1, 'fries': 1, 'other': 2, 'fatty': 1, 'snacks': 2, 'allows': 1, 'rank': 1, 'new': 1, 'views': 1, 'come': 1, 'tasty': 3, 'italian': 1, 'typically': 1, 'charges': 1, 'superb': 1, 'across': 1, 'home': 2, 'boost': 1, 'today': 1, 'score': 1, 'phoenix': 2, 'rater': 1, 'comes': 1, 'bad': 1, 'ok': 1, 'shopping': 3, 'entertainment': 1, 'hotels': 1, 'rating_xways': 1, 'carryout': 1, 'chopsticks': 1, 'available': 1, 'expert': 1, 'give': 6, 'towards': 1, 'reviewed': 2, 'known': 1, 'includes': 2, 'must': 1, 'modest': 2, 'fans': 1, 'arrangement': 1, 'english': 2, 'mcdonalds': 1, 'overlooking': 1, 'ideal': 1, 'setting': 3, 'choose': 1, 'worth': 1, 'had': 1, 'super': 1, 'ever': 1, 'won': 1, 'break': 1, 'then': 4, 'japanese': 1, 'recently': 1, 'scored': 1, 'parties': 1, 'ranked': 1, 'another': 1, 'falls': 1, 'here': 1, 'vaults': 1, 'amazing': 1, 'improving': 1, 'places': 1, 'beluga': 1, 'cam': 1, 'seafood': 1, 'exceeding': 1, 'focus': 1, 'law': 1, 'traveling': 1, 'provided': 1, 'trip': 1, 'able': 1, 'irresistible': 1, 'combo': 1, 'calm': 1, 'hungry': 1, 'beast': 1, 'including': 1, 'per': 1, 'head': 1, 'mark': 1, 'sorry': 1, 'treat': 1, 'yourself': 1, 'outlet': 1, 'prince': 1, 'might': 1, 'inn': 1, 'cricketers': 1, 'child': 1, 'cafe': 2, 'achieved': 1, 'spend': 1, 'upwards': 1, 'like': 1, 'rear': 1, 'opposite': 1, 'cocum': 1, 'pastas': 1, 'short': 1, 'distance': 1, 'highest': 2, 'takeaway': 2, 'experience': 1, 'urban': 1, 'middling': 1, 'soup': 1, 'spending': 1, 'stone': 1, 'throw': 1, 'stay': 1, 'mostly': 1, 'hearty': 1, 'previous': 1, 'said': 1, 'hall': 1, 'noodle': 1, 'sterling': 1, 'selections': 1, 'premium': 1, 'night': 1, 'restroom': 1, 'sell': 1, 'scenic': 1, 'surrounding': 1}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# GET DATA\n",
    "print(\"Reading...\")\n",
    "path = \"delexicalized/delex_only.csv\"\n",
    "raw_data = get_raw_data(path)\n",
    "input_data, mr_lang = get_input_data(path) #TODO: Correctly read all info of labels\n",
    "target_data, nl_lang = get_output_data(path)\n",
    "\n",
    "print(mr_lang.showLang())\n",
    "print(nl_lang.showLang())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=75):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=75):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):   \n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epoch, stop_early_count, learning_rate=0.01):\n",
    "    print(\"Started training!\")\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    best_loss = None # Keep track for early stopping\n",
    "    no_improv_count = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        for i in range(len(input_data)):\n",
    "            input_tensor = input_data[i].view(-1, 1)\n",
    "            target_tensor = target_data[i].view(-1, 1)\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            \n",
    "        print_loss_avg = print_loss_total / len(input_data)\n",
    "        \n",
    "        if best_loss == None or best_loss > print_loss_avg:\n",
    "            best_loss = print_loss_avg\n",
    "            no_improv_count = 0\n",
    "        else:\n",
    "            no_improv_count += 1\n",
    "            \n",
    "        \n",
    "        print_loss_total = 0\n",
    "        print('%s (Epoch: %d %d%%) %.4f' % (timeSince(start, epoch / n_epoch),\n",
    "                                     epoch, epoch / n_epoch * 100, print_loss_avg))\n",
    "\n",
    "        plot_loss_avg = plot_loss_total / len(input_data)\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        if no_improv_count == stop_early_count:\n",
    "                print(\"Stopping early...\")\n",
    "                break\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    print(\"Done!\")\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length=75):\n",
    "    with torch.no_grad():\n",
    "#         sentence = sentenceFromIndexes(mr_lang, input_tensor, delimeter=\", \")\n",
    "#         input_tensor = indexesFromSentence(mr_lang, sentence, delimeter=\", \")\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(nl_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        index = random.randint(0, len(input_data) - 1)\n",
    "        input_sentence = sentenceFromIndexes(mr_lang, input_data[index], delimeter=\", \")\n",
    "        target_sentence = sentenceFromIndexes(nl_lang, target_data[index], delimeter=\" \")\n",
    "        \n",
    "        print('>', input_sentence)\n",
    "        print('=', target_sentence)\n",
    "        output_words, attentions = evaluate(encoder, decoder, input_data[i])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training!\n",
      "2m 17s (- 227m 40s) (Epoch: 1 1%) 3.4968\n",
      "4m 36s (- 226m 1s) (Epoch: 2 2%) 3.1410\n",
      "6m 54s (- 223m 20s) (Epoch: 3 3%) 3.0108\n",
      "9m 13s (- 221m 17s) (Epoch: 4 4%) 2.9867\n",
      "11m 31s (- 219m 6s) (Epoch: 5 5%) 2.9004\n",
      "13m 51s (- 217m 10s) (Epoch: 6 6%) 2.8545\n",
      "16m 10s (- 214m 56s) (Epoch: 7 7%) 2.8679\n",
      "18m 30s (- 212m 49s) (Epoch: 8 8%) 2.7949\n",
      "20m 49s (- 210m 35s) (Epoch: 9 9%) 2.8076\n",
      "23m 10s (- 208m 35s) (Epoch: 10 10%) 2.7908\n",
      "25m 30s (- 206m 20s) (Epoch: 11 11%) 2.7763\n",
      "27m 49s (- 204m 5s) (Epoch: 12 12%) 2.7173\n",
      "30m 8s (- 201m 42s) (Epoch: 13 13%) 2.7630\n",
      "32m 27s (- 199m 23s) (Epoch: 14 14%) 2.6581\n",
      "34m 45s (- 197m 0s) (Epoch: 15 15%) 2.6636\n",
      "37m 4s (- 194m 40s) (Epoch: 16 16%) 2.6919\n",
      "39m 24s (- 192m 25s) (Epoch: 17 17%) 2.6596\n",
      "41m 43s (- 190m 3s) (Epoch: 18 18%) 2.6638\n",
      "44m 0s (- 187m 38s) (Epoch: 19 19%) 2.6135\n",
      "46m 18s (- 185m 15s) (Epoch: 20 20%) 2.5964\n",
      "48m 37s (- 182m 56s) (Epoch: 21 21%) 2.6127\n",
      "50m 55s (- 180m 31s) (Epoch: 22 22%) 2.6023\n",
      "53m 13s (- 178m 11s) (Epoch: 23 23%) 2.6006\n",
      "55m 30s (- 175m 46s) (Epoch: 24 24%) 2.6348\n",
      "57m 48s (- 173m 25s) (Epoch: 25 25%) 2.5639\n",
      "60m 5s (- 171m 2s) (Epoch: 26 26%) 2.5577\n",
      "62m 27s (- 168m 53s) (Epoch: 27 27%) 2.5825\n",
      "64m 50s (- 166m 43s) (Epoch: 28 28%) 2.5792\n",
      "67m 14s (- 164m 37s) (Epoch: 29 28%) 2.5672\n",
      "69m 39s (- 162m 31s) (Epoch: 30 30%) 2.5226\n",
      "72m 11s (- 160m 42s) (Epoch: 31 31%) 2.5458\n",
      "74m 35s (- 158m 30s) (Epoch: 32 32%) 2.5656\n",
      "76m 57s (- 156m 15s) (Epoch: 33 33%) 2.5008\n",
      "79m 18s (- 153m 57s) (Epoch: 34 34%) 2.5670\n",
      "81m 40s (- 151m 41s) (Epoch: 35 35%) 2.5056\n",
      "84m 4s (- 149m 28s) (Epoch: 36 36%) 2.5552\n",
      "86m 26s (- 147m 10s) (Epoch: 37 37%) 2.5369\n",
      "88m 47s (- 144m 52s) (Epoch: 38 38%) 2.5498\n",
      "Stopping early...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "early_stopping = 5\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(mr_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, nl_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "plot_losses = trainIters(encoder1, attn_decoder1, n_epoch, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> name[name_x], eattype[eattype_x], food[food_x], pricerange[pricerange_x], customer rating[customer rating_x], near[near_x], \n",
      "= name_x is a customer rating_xly rated food_x eattype_x near near_x . prices are average . \n",
      "< name_x eattype_x near near_x has a customer a star rating . prices pricerange_x . . rating_x of . . . . . food restaurant . . located near near_x . it has a price range of pricerange_x . . . . . . . priced food . food_x . . priced . food . . . . priced food_x food . . . priced . . . food . . . priced food . .\n",
      "\n",
      "> name[name_x], food[food_x], pricerange[pricerange_x], customer rating[customer rating_x], area[area_x], familyfriendly[yes], \n",
      "= food_x customer rating_x city . \n",
      "< name_x is a eattype_x near near_x . it serves food_x food food_x . food . pricerange_x . . . food . . . food . . . food . food_x food . is food is is . fast food at the pricerange_x . . nearby near_x . food_x food . it is be be food_x food . it is located in the city . it . called name_x . it has a price range of\n",
      "\n",
      "> name[name_x], food[food_x], pricerange[pricerange_x], area[area_x], familyfriendly[yes], near[near_x], \n",
      "= name_x serves food_x food in the pricerange_x price range and is located at the area_x near near_x and it is familyfriendly_x \n",
      "< name_x is a familyfriendly_x food_x in the area_x eattype_x near_x . it . . customer . . . . customer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . the area_x area . . . . . . . . the near_x . the it is familyfriendly_x and serves food_x food . the price range\n",
      "\n",
      "> name[name_x], eattype[eattype_x], food[food_x], pricerange[pricerange_x], \n",
      "= name_x is an food_x eattype_x that pricerange_x . \n",
      "< name_x is a area_x near near_x area_x eattype_x . food_x food . . pricerange_x . . . . . . . . . . . . . . . . . . . . the price range pricerange_x . pricerange_x . the pub is located near near_x and is very . . . in the area_x area close to the near_x . it . familyfriendly_x . . familyfriendly_x . . familyfriendly_x . . . familyfriendly_x\n",
      "\n",
      "> name[name_x], food[food_x], customer rating[customer rating_x], familyfriendly[yes], near[near_x], \n",
      "= there is familyfriendly_x food_x called name_x with a poor customer rating located near near_x . \n",
      "< name_x is located near near_x in the area_x it serves good food_x food . it . located in the area_x . customer rating_x customer near_x . . . . . . . . . . . . . . . . . . . . . . . . the to near_x . to be the food_x cuisine . to be familyfriendly_x . located near near_x in the area_x . . . . familyfriendly_x .\n",
      "\n",
      "> name[name_x], food[food_x], customer rating[customer rating_x], area[area_x], \n",
      "= along the area_x is name_x with an customer rating_x customer rating that sells food_x food \n",
      "< name_x is a restaurant restaurant near_x in the food_x . area_x . near near_x . . the . . . . . . . . . . . the for the area_x . . . . . . will for the area_x . . . . . will will be located near near_x . it will be be be food_x food . be be found in the pricerange_x price . . will be noted will\n",
      "\n",
      "> name[name_x], pricerange[pricerange_x], area[area_x], familyfriendly[no], \n",
      "= to get away pricerange_x name_x in the area_x . a meal there is upwards of  \n",
      "< name_x is a customer rating_x food_x food customer located in the area_x it it customer . . . . . . . . . . . . . . . . . . no kids . . . all age to young . is in the area_x area there is a familyfriendly_x . it is located in the area_x . . . . . . . . . . . . . the . .\n",
      "\n",
      "> name[name_x], food[food_x], pricerange[pricerange_x], near[near_x], \n",
      "= the name_x is a pricerange_x food_x restaurant near near_x . \n",
      "< name_x is area_x area_x serves food_x food for food_x food . familyfriendly_x pricerange_x . near near_x . familyfriendly_x . familyfriendly_x . familyfriendly_x . . kids . . . familyfriendly_x . . priced food_x menu . familyfriendly_x . to near_x . to be familyfriendly_x . near to near_x . the prices are pricerange_x . it is familyfriendly_x . is is familyfriendly_x . . is familyfriendly_x . . is familyfriendly_x . . priced food_x food . is\n",
      "\n",
      "> name[name_x], food[food_x], customer rating[customer rating_x], area[area_x], familyfriendly[no], \n",
      "= name_x has a great rating located in familyfriendly_x area_x food_x food \n",
      "< name_x is a eattype_x located in the area_x . customer rating_x the is customer rating_x to the north of the city center . and has a superb view across area_x doesn t food_x . . not familyfriendly_x . to young children . it is familyfriendly_x has a price range of . . . . . . . . . sushi . a familyfriendly_x . . . all the family young . . . . .\n",
      "\n",
      "> name[name_x], eattype[eattype_x], food[food_x], pricerange[pricerange_x], customer rating[customer rating_x], \n",
      "= food_x pricerange_x in the low price range with high customer ratings . \n",
      "< name_x is food_x restaurant in the the food_x area_x . . customer . . . customer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . priced . in the area_x area . . . . . . . . . . . . t . located in the area_x . the area_x is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This should actually use a validation set!!\n",
    "def generateOutputFile(encoder, decoder, file_name, amount=20):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "    \n",
    "        for i in range(amount):\n",
    "            index = random.randint(0, len(input_data) - 1)\n",
    "            \n",
    "            inputs.append(raw_data['mr'][index])\n",
    "\n",
    "            \n",
    "            input_sentence = sentenceFromIndexes(mr_lang, input_data[index], delimeter=\", \")\n",
    "            target_sentence = sentenceFromIndexes(nl_lang, target_data[index], delimeter=\" \")\n",
    "\n",
    "            output_words, attentions = evaluate(encoder, decoder, input_data[i])\n",
    "            output_sentence = ' '.join(output_words)\n",
    "            outputs.append(output_sentence)\n",
    "        \n",
    "        results = {\n",
    "            'mr': inputs,\n",
    "            'output': outputs\n",
    "        }\n",
    "        \n",
    "        df_results = pd.DataFrame(results, columns=['mr','output'])\n",
    "        df_results.to_csv(file_name, encoding='utf-8', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateOutputFile(encoder1, attn_decoder1, \"./delex_nl_output.csv\", amount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
